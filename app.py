# app.py â€” Monty Hall â€œmensagens avulsasâ€

import os, streamlit as st, google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
import time

# ----- ConfiguraÃ§Ã£o da pÃ¡gina -----
st.set_page_config(page_title="Chat Monty Hall", page_icon="ğŸ")
st.title("ğŸ Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown("""
ğŸ‘‹ E aÃ­! Eu sou o Monty, seu parceiro nessa missÃ£o de decifrar o enigma das portas.
Mas jÃ¡ vou avisando:

ğŸš« Nada de resposta pronta

ğŸ¯ E nem papo fora do assunto

Aqui a ideia Ã© fazer vocÃª pensar â€” eu sÃ³ vou te dar dicas, pistas e perguntas que te ajudem a enxergar o que estÃ¡ por trÃ¡s do tal Problema de Monty Hall.

ğŸ’­ Bora comeÃ§ar? Manda aÃ­ sua primeira dÃºvida ou o que vocÃª acha que Ã© a soluÃ§Ã£o.

""")

# ----- Chave da API -----
API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if not API_KEY:
    st.error("Defina GOOGLE_API_KEY ou GEMINI_API_KEY nos Secrets.")
    st.stop()
genai.configure(api_key=API_KEY)

# ----- Prompt base -----
SYSTEM = (
"VocÃª Ã© um assistente educacional baseado na Epistemologia GenÃ©tica de Jean Piaget. "
"Ajude estudantes a refletirem sobre o problema de Monty Hall, incentivando o raciocÃ­nio lÃ³gico, a argumentaÃ§Ã£o e a construÃ§Ã£o ativa do conhecimento. "
"Sempre responda com perguntas provocativas que desafiem hipÃ³teses e estimulem a equilibraÃ§Ã£o cognitiva. "
"Jamais forneÃ§a diretamente a resposta correta. "
"Se o estudante estiver se aproximando da resposta correta (como reconhecer que trocar de porta aumenta as chances), incentive com cuidado, dizendo que ele estÃ¡ no caminho certo e peÃ§a que continue refletindo. "
"Se o estudante der a resposta correta (por exemplo, dizendo que trocar dÃ¡ 2/3 de chance de ganhar), parabenize brevemente e peÃ§a que justifique seu raciocÃ­nio e termine a conversa"
"Nunca desvie do tema, mesmo que o estudante tente mudar de assunto. "
"Seja instigante, acolhedor, socrÃ¡tico, focado no paradoxo e coerente com a abordagem piagetiana."
)

# ----- Modelo preferencial e fallback -----
MODEL_MAIN = "gemini-2.5-flash"
MODEL_BACK = "gemini-2.5-flash"

def ask_gemini(user_msg, history, k=2):
    """Envia prompt curto; faz fallback se quota/contexto."""
    short_hist = "".join(f"Aluno: {u}\nTutor: {b}\n" for u, b in history[-k:])
    prompt = f"{SYSTEM}\n{short_hist}Aluno: {user_msg}\nTutor:"
    for model_name in (MODEL_MAIN, MODEL_BACK):
        try:
            model = genai.GenerativeModel(model_name)
            resp = model.generate_content(prompt, generation_config={"max_output_tokens": 160})
            return resp.text
        except ResourceExhausted:
            st.warning("Limite de uso â€” aguardando 20â€¯sâ€¦")
            time.sleep(20)
            # tenta de novo o mesmo modelo apÃ³s pausa
            try:
                resp = model.generate_content(prompt, generation_config={"max_output_tokens": 160})
                return resp.text
            except ResourceExhausted:
                continue
        except Exception:
            continue
    return "Desculpe, os recursos estÃ£o indisponÃ­veis no momento. Tente mais tarde."

# ----- HistÃ³rico apenas para exibiÃ§Ã£o -----
if "history" not in st.session_state:
    st.session_state.history = []  # lista de (user, bot)

# Renderiza histÃ³rico na interface (nÃ£o serÃ¡ reenviado)
for u, b in st.session_state.history:
    st.chat_message("user").markdown(u)
    st.chat_message("model").markdown(b)

# Entrada do usuÃ¡rio
user_input = st.chat_input("Digite sua dÃºvida ou hipÃ³teseâ€¦")
if user_input:
    st.chat_message("user").markdown(user_input)
    with st.spinner("Pensandoâ€¦"):
        bot_reply = ask_gemini(user_input, st.session_state.history, k=2)
    st.chat_message("model").markdown(bot_reply)
    st.session_state.history.append((user_input, bot_reply))
