import os
import time
import streamlit as st
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# ---------- Configura√ß√£o da p√°gina ----------
st.set_page_config(page_title="Chat Monty Hall", page_icon="üêê")
st.title("üêê Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown(
    """
üëã **E a√≠! Eu sou o Monty, seu parceiro no enigma das portas.**  

üö´ **Sem resposta pronta**‚ÄÉ|‚ÄÉüéØ **Sem papo fora do tema**  

Vou provocar sua mente com perguntas sobre o Problema de Monty‚ÄØHall.  

üí≠ **Bora come√ßar?** Mande a primeira d√∫vida ou hip√≥tese!
"""
)

# ---------- Chave da API ----------
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    st.error("GOOGLE_API_KEY n√£o definida. Adicione sua chave Gemini aos Secrets.")
    st.stop()

genai.configure(api_key=API_KEY)

# ---------- Prompt base (piagetiano) ----------
SYSTEM = (
    "Voc√™ √© um assistente educacional inspirado em Jean Piaget. "
    "Ajude estudantes a refletir sobre o paradoxo de Monty Hall com perguntas que provoquem desequil√≠brio cognitivo. "
    "Jamais forne√ßa a resposta direta. Se o estudante se aproximar, incentive; se acertar, parabenize e pe√ßa a justificativa. "
    "Nunca saia do tema, mesmo que o usu√°rio tente desviar. Mantenha tom gentil e instigante."
)

# ---------- Modelos ----------
model_flash = genai.GenerativeModel("models/gemini-1.5-flash-latest")
model_pro   = genai.GenerativeModel("gemini-pro")  # fallback

# ---------- Fun√ß√£o de consulta ----------
def ask_gemini(user_msg: str, history: list[tuple[str, str]], k: int = 2) -> str:
    """Envia prompt ao Gemini com um hist√≥rico curto."""
    short_context = "".join(f"Aluno: {u}\nTutor: {b}\n" for u, b in history[-k:])
    prompt = f"{SYSTEM}\n{short_context}Aluno: {user_msg}\nTutor:"
    try:
        resp = model_flash.generate_content(
            prompt, generation_config={"max_output_tokens": 180}
        )
        return resp.text
    except ResourceExhausted:
        st.warning("Limite atingido. Aguardando 20‚ÄØs e tentando novamente‚Ä¶")
        time.sleep(20)
        resp = model_pro.generate_content(
            prompt, generation_config={"max_output_tokens": 150}
        )
        return resp.text

# ---------- Estado da sess√£o ----------
if "history" not in st.session_state:
    st.session_state.history = []  # lista de (user, bot)

# ---------- Renderiza hist√≥rico ----------
for user_msg, bot_msg in st.session_state.history:
    st.chat_message("user").markdown(user_msg)
    st.chat_message("model").markdown(bot_msg)

# ---------- Entrada do usu√°rio ----------
user_input = st.chat_input("Digite sua pergunta ou hip√≥tese sobre Monty Hall‚Ä¶")

if user_input:
    st.chat_message("user").markdown(user_input)

    with st.spinner("Pensando‚Ä¶"):
        bot_reply = ask_gemini(user_input, st.session_state.history, k=2)

    st.chat_message("model").markdown(bot_reply)
    st.session_state.history.append((user_input, bot_reply))
