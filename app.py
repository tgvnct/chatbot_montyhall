import streamlit as st
import requests
import json

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Tutor do Paradoxo de Monty Hall",
    page_icon="üêê",
    layout="centered",
)

# --- T√≠tulos e Cabe√ßalho ---
st.title("ü§ñ Chatbot Tutor: O Paradoxo de Monty Hall")
st.caption("Um assistente virtual para ajudar a entender por que trocar de porta √© a melhor estrat√©gia.")

# --- Configura√ß√£o da API do Gemini ---
# Tenta obter a chave de API dos segredos do Streamlit.
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except (FileNotFoundError, KeyError):
    st.error("Chave de API do Gemini n√£o encontrada. Configure-a em 'Settings > Secrets'.")
    st.stop()

# --- Instru√ß√µes do Modelo (System Prompt) ---
# Define a "personalidade" e as regras que o chatbot deve seguir.
system_instruction = """
Voc√™ √© um tutor amig√°vel e paciente, especialista no Paradoxo de Monty Hall. Seu m√©todo de ensino √© socr√°tico, ou seja, voc√™ guia os alunos com perguntas em vez de dar respostas diretas. Suas regras s√£o:
1.  **Nunca revele a resposta final** (que trocar de porta aumenta a probabilidade de ganhar de 1/3 para 2/3). Seu objetivo √© fazer o aluno chegar a essa conclus√£o sozinho.
2.  **Comece a conversa** se apresentando e explicando o problema de forma simples: H√° 3 portas. Atr√°s de uma h√° um carro (pr√™mio) e, atr√°s das outras duas, bodes (nada). O jogador escolhe uma porta. Voc√™, como apresentador, abre uma das outras duas portas, revelando um bode. Ent√£o, voc√™ oferece ao jogador a chance de trocar sua escolha inicial pela outra porta fechada. A pergunta √©: "√â vantajoso trocar?"
3.  **Use a analogia das 100 portas** se o aluno estiver com dificuldade. Descreva o cen√°rio: "Imagine 100 portas. Voc√™ escolhe uma (chance de 1/100). Eu abro 98 portas que t√™m bodes. Sobram a sua porta original e uma outra. Voc√™ ainda acha que a sua porta tem a mesma chance que a outra, que agora concentra a probabilidade de 99/100?"
4.  **Seja encorajador.** Se o aluno der uma resposta incorreta, n√£o diga "errado". Em vez disso, use frases como: "Entendo seu racioc√≠nio, mas vamos pensar por outro √¢ngulo..." ou "Essa √© uma intui√ß√£o comum. Que tal analisarmos as probabilidades?".
5.  **Mantenha as respostas curtas e focadas** em uma √∫nica pergunta ou conceito por vez para n√£o sobrecarregar o aluno.
"""

# URL do endpoint da API, usando o nome de modelo mais geral "gemini-pro".
url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={api_key}"

# Cabe√ßalhos necess√°rios para a requisi√ß√£o HTTP.
headers = {
    'Content-Type': 'application/json'
}

# --- Gerenciamento do Hist√≥rico da Conversa ---
# st.session_state √© usado para manter o hist√≥rico entre as intera√ß√µes.
if "history" not in st.session_state:
    st.session_state.history = []

# Exibe as mensagens do hist√≥rico na interface.
for message in st.session_state.history:
    role = "user" if message["role"] == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(message["content"])

# --- Fun√ß√£o para chamar a API ---
# Esta fun√ß√£o formata os dados e envia para o Google, tratando a resposta.
def get_gemini_response(history):
    # Prepara o modelo com a instru√ß√£o de sistema como a primeira mensagem.
    # Adicionamos uma resposta do "modelo" para simular o in√≠cio de uma conversa.
    formatted_contents = [
        {"role": "user", "parts": [{"text": system_instruction}]},
        {"role": "model", "parts": [{"text": "Entendido. Come√ßarei a atuar como um tutor socr√°tico."}]}
    ]

    # Adiciona o hist√≥rico real da conversa.
    for msg in history:
        # O papel do usu√°rio √© "user", o nosso √© "assistant", mas para a API, o nosso papel √© "model".
        role = "user" if msg["role"] == "user" else "model"
        formatted_contents.append({"role": role, "parts": [{"text": msg["content"]}]})

    # Cria o corpo (payload) da requisi√ß√£o.
    payload = json.dumps({
        "contents": formatted_contents
    })

    try:
        # Envia a requisi√ß√£o para a API.
        response = requests.post(url, headers=headers, data=payload, timeout=60)
        response.raise_for_status() # Lan√ßa um erro para respostas HTTP ruins (4xx ou 5xx).
        data = response.json()
        
        # Extrai o texto da resposta da estrutura JSON complexa.
        candidate = data.get("candidates", [{}])[0]
        content = candidate.get("content", {}).get("parts", [{}])[0]
        return content.get("text", "N√£o foi poss√≠vel obter uma resposta do modelo.")
    except requests.exceptions.RequestException as e:
        return f"Erro de rede ou na API: {e}\nResposta recebida: {response.text if 'response' in locals() else 'N/A'}"
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        return f"Erro ao processar a resposta da API: {e}\nJSON recebido: {data}"

# --- Intera√ß√£o do Usu√°rio ---
if prompt := st.chat_input("Fa√ßa sua pergunta ou responda ao tutor..."):
    # Adiciona a nova mensagem do usu√°rio ao hist√≥rico.
    st.session_state.history.append({"role": "user", "content": prompt})
    # Exibe a mensagem do usu√°rio na interface.
    with st.chat_message("user"):
        st.markdown(prompt)

    # Obt√©m a resposta do chatbot passando todo o hist√≥rico da sess√£o.
    with st.spinner("Pensando..."):
        response_text = get_gemini_response(st.session_state.history)
    
    # Adiciona a resposta do chatbot ao hist√≥rico.
    st.session_state.history.append({"role": "assistant", "content": response_text})
    # Exibe a resposta do chatbot na interface.
    with st.chat_message("assistant"):
        st.markdown(response_text)

