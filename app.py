# app.py â€” Monty Hall â€œmensagens avulsasâ€

import os, streamlit as st, google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
import time

# ----- ConfiguraÃ§Ã£o da pÃ¡gina -----
st.set_page_config(page_title="Chat Monty Hall", page_icon="ğŸ")
st.title("ğŸ Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown("""
ğŸ‘‹ E aÃ­! Eu sou o Monty, seu parceiro nessa missÃ£o de decifrar o enigma das portas.
Mas jÃ¡ vou avisando:

ğŸš« Nada de resposta pronta

ğŸ¯ E nem papo fora do assunto

Aqui a ideia Ã© fazer vocÃª pensar â€” eu sÃ³ vou te dar dicas, pistas e perguntas que te ajudem a enxergar o que estÃ¡ por trÃ¡s do tal Problema de Monty Hall.

ğŸ’­ Bora comeÃ§ar? Manda aÃ­ sua primeira dÃºvida ou o que vocÃª acha que Ã© a soluÃ§Ã£o.

""")

# ----- Chave da API -----
API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if not API_KEY:
    st.error("Defina GOOGLE_API_KEY ou GEMINI_API_KEY nos Secrets.")
    st.stop()
genai.configure(api_key=API_KEY)

# ----- Prompt base -----
SYSTEM = (
    "VocÃª Ã© um assistente educacional inspirado em Piaget. "
    "Ajude estudantes a refletir sobre o paradoxo de Monty Hall com perguntas provocativas. "
    "Nunca forneÃ§a a resposta direta. Incentive justificativas. "
    "Mantenha o foco no tema, mesmo que o aluno tente desviar."
)

# ----- Modelo preferencial e fallback -----
MODEL_MAIN = "models/gemini-1.5-flash-latest"
MODEL_BACK = "models/gemini-1.0-pro-latest"

def ask_gemini(user_msg, history, k=2):
    """Envia prompt curto; faz fallback se quota/contexto."""
    short_hist = "".join(f"Aluno: {u}\nTutor: {b}\n" for u, b in history[-k:])
    prompt = f"{SYSTEM}\n{short_hist}Aluno: {user_msg}\nTutor:"
    for model_name in (MODEL_MAIN, MODEL_BACK):
        try:
            model = genai.GenerativeModel(model_name)
            resp = model.generate_content(prompt, generation_config={"max_output_tokens": 160})
            return resp.text
        except ResourceExhausted:
            st.warning("Limite de uso â€” aguardando 20â€¯sâ€¦")
            time.sleep(20)
            # tenta de novo o mesmo modelo apÃ³s pausa
            try:
                resp = model.generate_content(prompt, generation_config={"max_output_tokens": 160})
                return resp.text
            except ResourceExhausted:
                continue
        except Exception:
            continue
    return "Desculpe, os recursos estÃ£o indisponÃ­veis no momento. Tente mais tarde."

# ----- HistÃ³rico apenas para exibiÃ§Ã£o -----
if "history" not in st.session_state:
    st.session_state.history = []  # lista de (user, bot)

# Renderiza histÃ³rico na interface (nÃ£o serÃ¡ reenviado)
for u, b in st.session_state.history:
    st.chat_message("user").markdown(u)
    st.chat_message("model").markdown(b)

# Entrada do usuÃ¡rio
user_input = st.chat_input("Digite sua dÃºvida ou hipÃ³teseâ€¦")
if user_input:
    st.chat_message("user").markdown(user_input)
    with st.spinner("Pensandoâ€¦"):
        bot_reply = ask_gemini(user_input, st.session_state.history, k=2)
    st.chat_message("model").markdown(bot_reply)
    st.session_state.history.append((user_input, bot_reply))
