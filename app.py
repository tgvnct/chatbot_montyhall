# app.pyÂ â€”Â Chatbot Montyâ€¯Hall com GeminiÂ API (versÃ£o segura)
# Autor: TiagoÂ +Â ChatGPTÂ |Â RevisÃ£o: julâ€¯2025
# -----------------------------------------------------------
# â€¢ MantÃ©m no mÃ¡ximo MAX_TURNS trocas para nÃ£o estourar contexto.
# â€¢ Captura ResourceExhausted e reenvia a pergunta sem histÃ³rico.
# â€¢ Usa GOOGLE_API_KEY em secrets.toml / painel de segredos.
# -----------------------------------------------------------

import os
import streamlit as st
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# --------------- CONFIG. DA PÃGINA --------------- #
st.set_page_config(page_title="Chat Monty Hall", page_icon="ğŸ")
st.title("ğŸ Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown(
    """
ğŸ‘‹ **E aÃ­! Eu sou o Monty, seu parceiro no enigma das portas.**  

ğŸš« **Sem resposta pronta**â€ƒ|â€ƒğŸ¯ **Sem papo fora do tema**  

Vou lanÃ§ar perguntas e pistas para mexer nas suas hipÃ³teses sobre o Problema de Montyâ€¯Hall.  

ğŸ’­ **Bora comeÃ§ar?** Mande sua primeira ideia ou dÃºvida!
"""
)

# --------------- CHAVE DA API --------------- #
API_KEY = os.getenv("GOOGLE_API_KEY")  # defina em secrets.toml ou no painel Secrets
if not API_KEY:
    st.error("GOOGLE_API_KEY nÃ£o definida. Adicione sua chave Gemini aos Secrets.")
    st.stop()

genai.configure(api_key=API_KEY)

# --------------- PROMPT DE SISTEMA --------------- #
system_instruction = (
    "VocÃª Ã© um assistente educacional inspirado em Jean Piaget. "
    "Ajude estudantes a refletir sobre o paradoxo de Monty Hall com perguntas que provoquem desequilÃ­brio cognitivo. "
    "Jamais forneÃ§a a resposta direta. Se o estudante se aproximar, incentive; se acertar, parabenize e peÃ§a a justificativa. "
    "Nunca saia do tema mesmo que o usuÃ¡rio tente desviar. Mantenha tom gentil e instigante."
)

# --------------- MODELO --------------- #
model = genai.GenerativeModel(
    model_name="models/gemini-1.5-flash-latest",
    system_instruction=system_instruction,
)

# --------------- CONTEXTO / HISTÃ“RICO --------------- #
MAX_TURNS = 12  # mÃ¡ximo de trocas antes de reiniciar

if "chat" not in st.session_state:
    st.session_state.chat = model.start_chat()
    st.session_state.turns = 0

# Exibe histÃ³rico existente
for msg in st.session_state.chat.history:
    with st.chat_message(msg.role):
        st.markdown(msg.parts[0].text)

# --------------- ENTRADA DO USUÃRIO --------------- #
user_input = st.chat_input("Digite sua pergunta ou hipÃ³tese sobre Monty Hallâ€¦")

if user_input:
    # Mostra mensagem do usuÃ¡rio
    st.chat_message("user").markdown(user_input)

    # Incrementa e verifica limite de turnos
    st.session_state.turns += 1
    if st.session_state.turns > MAX_TURNS:
        st.session_state.chat = model.start_chat()
        st.session_state.turns = 1  # conta a interaÃ§Ã£o atual

    # Tenta enviar normalmente
    try:
        with st.chat_message("model"):
            response = st.session_state.chat.send_message(user_input)
            st.markdown(response.text)

    # Se contexto/quota estourar, reinicia e reenvia sem histÃ³rico
    except ResourceExhausted:
        st.warning("ğŸ“‰ Limite de contexto ou quota excedido. Reiniciando conversaâ€¦")
        st.session_state.chat = model.start_chat()
        st.session_state.turns = 1
        with st.chat_message("model"):
            safe_response = model.generate_content(user_input)
            st.markdown(safe_response.text)
