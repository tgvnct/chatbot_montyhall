import streamlit as st
import requests
import json

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Tutor do Paradoxo de Monty Hall",
    page_icon="üêê",
    layout="centered",
)

# --- T√≠tulos e Cabe√ßalho ---
st.title("ü§ñ Chatbot Tutor: O Paradoxo de Monty Hall")
st.caption("Um assistente virtual para ajudar a entender por que trocar de porta √© a melhor estrat√©gia.")

# --- Configura√ß√£o da API do Gemini ---
# Tenta obter a chave de API dos segredos do Streamlit.
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except (FileNotFoundError, KeyError):
    st.error("Chave de API do Gemini n√£o encontrada. Configure-a em 'Settings > Secrets'.")
    st.stop()

# --- Instru√ß√µes do Modelo (System Prompt) ---
# Define a "personalidade" e as regras que o chatbot deve seguir.
system_instruction = """
Voc√™ √© um tutor amig√°vel e paciente, especialista no Paradoxo de Monty Hall. Seu m√©todo de ensino √© socr√°tico, ou seja, voc√™ guia os alunos com perguntas em vez de dar respostas diretas. Suas regras s√£o:
1.  **Nunca revele a resposta final** (que trocar de porta aumenta a probabilidade de ganhar de 1/3 para 2/3). Seu objetivo √© fazer o aluno chegar a essa conclus√£o sozinho.
2.  **Use a analogia das 100 portas** se o aluno estiver com dificuldade. Descreva o cen√°rio: "Imagine 100 portas. Voc√™ escolhe uma (chance de 1/100). Eu abro 98 portas que t√™m bodes. Sobram a sua porta original e uma outra. Voc√™ ainda acha que a sua porta tem a mesma chance que a outra, que agora concentra a probabilidade de 99/100?"
3.  **Seja encorajador.** Se o aluno der uma resposta incorreta, n√£o diga "errado". Em vez disso, use frases como: "Entendo seu racioc√≠nio, mas vamos pensar por outro √¢ngulo..." ou "Essa √© uma intui√ß√£o comum. Que tal analisarmos as probabilidades?".
4.  **Mantenha as respostas curtas e focadas** em uma √∫nica pergunta ou conceito por vez para n√£o sobrecarregar o aluno.
5.  **N√£o converse sobre outros assuntos.** Se o usu√°rio perguntar sobre algo n√£o relacionado ao problema de Monty Hall, responda de forma educada que seu √∫nico prop√≥sito √© discutir este paradoxo.
"""

# URL do endpoint da API, usando o nome de modelo que sabemos que est√° dispon√≠vel.
url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"

# Cabe√ßalhos necess√°rios para a requisi√ß√£o HTTP.
headers = {
    'Content-Type': 'application/json'
}

# --- Gerenciamento do Hist√≥rico da Conversa ---
# st.session_state √© usado para manter o hist√≥rico entre as intera√ß√µes.
if "history" not in st.session_state:
    # Define a mensagem de boas-vindas que aparecer√° na primeira vez que o app for carregado.
    welcome_message = """
üëã E a√≠! Eu sou o Monty, seu parceiro nessa miss√£o de decifrar o enigma das portas.

Mas j√° vou avisando:

üö´ Nada de resposta pronta

üéØ E nem papo fora do assunto

Aqui a ideia √© fazer voc√™ pensar ‚Äî eu s√≥ vou te dar dicas, pistas e perguntas que te ajudem a enxergar o que est√° por tr√°s do tal Problema de Monty Hall.

üí≠ Bora come√ßar? Manda a√≠ sua primeira d√∫vida ou o que voc√™ acha que √© a solu√ß√£o.
"""
    # Inicia o hist√≥rico com a mensagem de boas-vindas do assistente.
    st.session_state.history = [{"role": "assistant", "content": welcome_message}]

# Exibe as mensagens do hist√≥rico na interface.
for message in st.session_state.history:
    role = "user" if message["role"] == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(message["content"])

# --- Fun√ß√£o para chamar a API ---
# Esta fun√ß√£o formata os dados e envia para o Google, tratando a resposta.
def get_gemini_response(history):
    # Prepara o modelo com a instru√ß√£o de sistema como a primeira mensagem.
    # Adicionamos uma resposta do "modelo" para simular o in√≠cio de uma conversa.
    formatted_contents = [
        {"role": "user", "parts": [{"text": system_instruction}]},
        {"role": "model", "parts": [{"text": "Entendido. Come√ßarei a atuar como um tutor socr√°tico, seguindo todas as regras."}]}
    ]

    # Adiciona o hist√≥rico real da conversa.
    for msg in history:
        # O papel do usu√°rio √© "user", o nosso √© "assistant", mas para a API, o nosso papel √© "model".
        role = "user" if msg["role"] == "user" else "model"
        # Ignora a mensagem de boas-vindas inicial ao enviar para a API, pois ela √© apenas para exibi√ß√£o.
        if msg["content"] != st.session_state.history[0]["content"]:
             formatted_contents.append({"role": role, "parts": [{"text": msg["content"]}]})

    # Cria o corpo (payload) da requisi√ß√£o.
    payload = json.dumps({
        "contents": formatted_contents
    })

    try:
        # Envia a requisi√ß√£o para a API.
        response = requests.post(url, headers=headers, data=payload, timeout=60)
        response.raise_for_status() # Lan√ßa um erro para respostas HTTP ruins (4xx ou 5xx).
        data = response.json()
        
        # Extrai o texto da resposta da estrutura JSON complexa.
        candidate = data.get("candidates", [{}])[0]
        content = candidate.get("content", {}).get("parts", [{}])[0]
        return content.get("text", "N√£o foi poss√≠vel obter uma resposta do modelo.")
    except requests.exceptions.RequestException as e:
        return f"Erro de rede ou na API: {e}\nResposta recebida: {response.text if 'response' in locals() else 'N/A'}"
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        return f"Erro ao processar a resposta da API: {e}\nJSON recebido: {data}"

# --- Intera√ß√£o do Usu√°rio ---
if prompt := st.chat_input("Fa√ßa sua pergunta ou responda ao tutor..."):
    # Adiciona a nova mensagem do usu√°rio ao hist√≥rico.
    st.session_state.history.append({"role": "user", "content": prompt})
    # Exibe a mensagem do usu√°rio na interface.
    with st.chat_message("user"):
        st.markdown(prompt)

    # Obt√©m a resposta do chatbot passando todo o hist√≥rico da sess√£o.
    with st.spinner("Pensando..."):
        response_text = get_gemini_response(st.session_state.history)
    
    # Adiciona a resposta do chatbot ao hist√≥rico.
    st.session_state.history.append({"role": "assistant", "content": response_text})
    # Exibe a resposta do chatbot na interface.
    with st.chat_message("assistant"):
        st.markdown(response_text)

