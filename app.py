# app.py ‚Äî Chatbot Monty‚ÄØHall (vers√£o sem chat‚ÄëAPI)
# ----------------------------------------------------------
# ‚Ä¢ Cada chamada usa generate_content() sem hist√≥rico pesado.
# ‚Ä¢ Inclui apenas as 2 √∫ltimas trocas no prompt para evitar estourar contexto.
# ‚Ä¢ Back‚Äëoff de 20‚ÄØs + fallback para modelo gemini‚Äëpro se ResourceExhausted.
# ‚Ä¢ Usa GOOGLE_API_KEY nos Secrets ou vari√°vel de ambiente.
# ----------------------------------------------------------

import os
import time
import streamlit as st
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# --------------- CONFIG. DA P√ÅGINA --------------- #
st.set_page_config(page_title="Chat Monty Hall", page_icon="üêê")
st.title("üêê Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown(
    """
üëã **E a√≠! Eu sou o Monty, seu parceiro no enigma das portas.**  

üö´ **Sem resposta pronta**‚ÄÉ|‚ÄÉüéØ **Sem papo fora do tema**  

Vou provocar sua mente com perguntas sobre o Problema de Monty‚ÄØHall.  

üí≠ **Bora come√ßar?** Mande a primeira d√∫vida ou hip√≥tese!
"""
)

# --------------- CHAVE DA API --------------- #
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    st.error("GOOGLE_API_KEY n√£o definida. Adicione sua chave Gemini aos Secrets.")
    st.stop()

genai.configure(api_key=API_KEY)

# --------------- PROMPT BASE (PIAGETIANO) --------------- #
SYSTEM_PROMPT = (
    "Voc√™ √© um assistente educacional inspirado em Jean Piaget. "
    "Ajude estudantes a refletir sobre o paradoxo de Monty Hall com perguntas que provoquem desequil√≠brio cognitivo. "
    "Jamais forne√ßa a resposta direta. Se o estudante se aproximar, incentive; se acertar, parabenize e pe√ßa a justificativa. "
    "Nunca saia do tema, mesmo que o usu√°rio tente desviar. Mantenha tom gentil e instigante."
)

# --------------- FUN√á√ïES AUXILIARES --------------- #

def build_prompt(user_msg: str, history: list[tuple[str, str]], k: int = 2) -> str:
    """Monta o prompt com SYSTEM + √∫ltimas k trocas + pergunta atual."""
    context = "".join(f"Aluno: {u}\nTutor: {b}\n" for u, b in history[-k:])
    return f"{SYSTEM_PROMPT}\n{context}Aluno: {user_msg}\nTutor:"


def ask_gemini(prompt: str) -> str:
    """Envia prompt; tenta flash e faz fallback se necess√°rio."""
    try:
        response = genai.generate_content(
            model="models/gemini-1.5-flash-latest",
            prompt=prompt,
            generation_config={"max_output_tokens": 180},
        )
        return response.text
    except ResourceExhausted:
        time.sleep(20)  # espera e tenta modelo mais leve
        response = genai.generate_content(
            model="gemini-pro",
            prompt=prompt,
            generation_config={"max_output_tokens": 150},
        )
        return response.text

# --------------- ESTADO DA SESS√ÉO --------------- #
if "history" not in st.session_state:
    st.session_state.history = []  # lista de tuplas (user, bot)

# --------------- RENDERIZA HIST√ìRICO --------------- #
for u_msg, b_msg in st.session_state.history:
    st.chat_message("user").markdown(u_msg)
    st.chat_message("model").markdown(b_msg)

# --------------- ENTRADA DO USU√ÅRIO --------------- #
user_input = st.chat_input("Digite sua pergunta ou hip√≥tese sobre Monty Hall‚Ä¶")

if user_input:
    st.chat_message("user").markdown(user_input)

    prompt = build_prompt(user_input, st.session_state.history, k=2)

    with st.spinner("Pensando‚Ä¶"):
        bot_reply = ask_gemini(prompt)

    st.chat_message("model").markdown(bot_reply)
    st.session_state.history.append((user_input, bot_reply))
