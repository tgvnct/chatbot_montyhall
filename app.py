import os
import time
import streamlit as st
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# --------------- CONFIG. DA P√ÅGINA --------------- #
st.set_page_config(page_title="Chat Monty Hall", page_icon="üêê")
st.title("üêê Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown(
    """
üëã **E a√≠! Eu sou o Monty, seu parceiro no enigma das portas.**  

üö´ **Sem resposta pronta**‚ÄÉ|‚ÄÉüéØ **Sem papo fora do tema**  

Vou provocar sua mente com perguntas sobre o Problema de Monty‚ÄØHall.  

üí≠ **Bora come√ßar?** Mande a primeira d√∫vida ou hip√≥tese!
"""
)

# --------------- CHAVE DA API --------------- #
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    st.error("GOOGLE_API_KEY n√£o definida. Adicione a chave Gemini nos Secrets.")
    st.stop()

genai.configure(api_key=API_KEY)

# --------------- PROMPT BASE (PIAGETIANO) --------------- #
SYSTEM_PROMPT = (
    "Voc√™ √© um assistente educacional inspirado em Jean¬†Piaget. "
    "Ajude estudantes a refletir sobre o paradoxo de Monty¬†Hall com perguntas que provoquem desequil√≠brio cognitivo. "
    "Jamais forne√ßa a resposta direta. Se o estudante se aproximar, incentive; se acertar, parabenize e pe√ßa a justificativa. "
    "Nunca saia do tema, mesmo que o usu√°rio tente desviar. Mantenha tom gentil e instigante."
)

# --------------- FUN√á√ÉO HELPERS --------------- #

def build_prompt(user_msg: str, history: list[tuple[str, str]], k: int = 2) -> str:
    """Retorna string: SYSTEM + √∫ltimas k trocas + pergunta atual."""
    context = "".join(
        f"Aluno: {u}\nTutor: {b}\n" for u, b in history[-k:]
    )
    return f"{SYSTEM_PROMPT}\n{context}Aluno: {user_msg}\nTutor:"


def ask_gemini(prompt: str):
    """Envia prompt; tenta flash‚Üípro; trata ResourceExhausted."""
    try:
        resp = genai.generate_content(
            model="models/gemini-1.5-flash-latest",
            prompt=prompt,
            generation_config={"max_output_tokens": 180},
        )
        return resp.text
    except ResourceExhausted:
        # back‚Äëoff simples (quota/minute) + fallback p/ modelo menor
        time.sleep(20)
        resp = genai.generate_content(
            model="gemini-pro",  # menos exigente de cota
            prompt=prompt,
            generation_config={"max_output_tokens": 150},
        )
        return resp.text

# --------------- ESTADO DA SESS√ÉO --------------- #
if "history" not in st.session_state:
    st.session_state.history = []  # lista de (user, bot)

# Renderiza hist√≥rico
for u, b in st.session_state.history:
    st.chat_message("user").markdown(u)
    st.chat_message("model").markdown(b)

# Entrada
user_input = st.chat_input("Digite sua pergunta ou hip√≥tese sobre Monty Hall‚Ä¶")

if user_input:
    # Mostra imediatamente
    st.chat_message("user").markdown(user_input)

    # Monta prompt curto
    prompt = build_prompt(user_input, st.session_state.history, k=2)

    # Chama Gemini
    with st.spinner("Pensando‚Ä¶"):
        bot_reply = ask_gemini(prompt)

    st.chat_message("model").markdown(bot_reply)

    # Atualiza hist√≥rico
    st.session_state.history.append((user_input, bot_reply))
```
