# app.py ‚Äî Chatbot Monty Hall com truncamento de hist√≥rico

import os
import streamlit as st
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# ---------------- CONFIGURA√á√ÉO DA P√ÅGINA ---------------- #
st.set_page_config(page_title="Chat Monty Hall", page_icon="üé≤")
st.title("üêê Chatbot: Reflita sobre o Paradoxo de Monty Hall")

st.markdown(
    """
üëã **E a√≠! Eu sou o Monty, seu parceiro nessa miss√£o de decifrar o enigma das portas.**  

üö´ **Nada de resposta pronta**  
üéØ **E nem papo fora do assunto**  

Aqui a ideia √© fazer voc√™ pensar¬†‚Äî s√≥ vou te dar dicas, pistas e perguntas que ajudem a enxergar o que est√° por tr√°s do Problema de Monty¬†Hall.  

üí≠ **Bora come√ßar?** Manda a√≠ sua primeira d√∫vida ou o que voc√™ acha que √© a solu√ß√£o.
"""
)

# ---------------- CHAVE DA API GOOGLE GEMINI ---------------- #
API_KEY = os.getenv("GOOGLE_API_KEY")  # definida em secrets.toml ou no painel Secrets
if not API_KEY:
    st.error("Chave da API Gemini n√£o encontrada. Defina GOOGLE_API_KEY nos Secrets.")
    st.stop()

genai.configure(api_key=API_KEY)

# ---------------- INSTRU√á√ÉO SIST√äMICA ---------------- #
system_instruction = (
    "Voc√™ √© um assistente educacional baseado na Epistemologia Gen√©tica de Jean Piaget. "
    "Ajude estudantes a refletirem sobre o problema de Monty Hall, incentivando o racioc√≠nio l√≥gico, a argumenta√ß√£o e a constru√ß√£o ativa do conhecimento. "
    "Sempre responda com perguntas provocativas que desafiem hip√≥teses e estimulem a equilibra√ß√£o cognitiva. "
    "Jamais forne√ßa diretamente a resposta correta. "
    "Se o estudante se aproximar da resposta, incentive; se acertar, parabenize e pe√ßa justificativa."
    "Se o estudante der a resposta certa, parabenize e termine a conversa"
    "Nunca desvie do tema, mesmo que o estudante tente mudar de assunto. "
    "Seja instigante, acolhedor e focado no paradoxo."
)

# ---------------- MODELO ---------------- #
model = genai.GenerativeModel(
    model_name="models/gemini-1.5-flash-latest",
    system_instruction=system_instruction,
)

# ---------------- CONSTANTES ---------------- #
MAX_HISTORY = 10  # mant√©m apenas as 10 √∫ltimas intera√ß√µes (user + model)

# ---------------- INICIALIZA SESS√ÉO ---------------- #
if "chat" not in st.session_state:
    st.session_state.chat = model.start_chat()

# ---------------- EXIBE HIST√ìRICO EXISTENTE ---------------- #
for msg in st.session_state.chat.history:
    with st.chat_message(msg.role):
        st.markdown(msg.parts[0].text)

# ---------------- ENTRADA DO USU√ÅRIO ---------------- #
user_input = st.chat_input("Digite sua pergunta ou ideia sobre Monty Hall...")
if user_input:
    st.chat_message("user").markdown(user_input)

    # ----- TRUNCA HIST√ìRICO PARA EVITAR ResourceExhausted ----- #
    if len(st.session_state.chat.history) > 2 * MAX_HISTORY:
        st.session_state.chat.history = st.session_state.chat.history[-2 * MAX_HISTORY :]

    # ----- ENVIA MENSAGEM COM TRATAMENTO DE ERRO ----- #
    try:
        with st.chat_message("model"):
            response = st.session_state.chat.send_message(user_input)
            st.markdown(response.text)
    except ResourceExhausted:
        st.warning(
            "Limite de contexto ou quota atingido. O hist√≥rico foi reiniciado para continuar a conversa."
        )
        st.session_state.chat = model.start_chat()  # zera hist√≥rico
        with st.chat_message("model"):
            response = st.session_state.chat.send_message(user_input)
            st.markdown(response.text)
