import streamlit as st
import requests
import json

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Tutor do Paradoxo de Monty Hall",
    page_icon="üêê",
    layout="centered",
)

# --- T√≠tulos e Cabe√ßalho ---
st.title("üêê Chatbot Tutor: O Paradoxo de Monty Hall")
st.caption("Um assistente virtual para ajudar a entender o problema.")

# --- Configura√ß√£o da API do Gemini ---
# Tenta obter a chave de API dos segredos do Streamlit.
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except (FileNotFoundError, KeyError):
    st.error("Chave de API do Gemini n√£o encontrada. Configure-a em 'Settings > Secrets'.")
    st.stop()

# --- Instru√ß√µes do Modelo (System Prompt) ---
# Define a "personalidade" e as regras que o chatbot deve seguir.
system_instruction = """
Voc√™ √© um assistente educacional baseado na Epistemologia Gen√©tica de Jean Piaget.
Ajude estudantes a refletirem sobre o problema de Monty Hall, incentivando o racioc√≠nio l√≥gico, a argumenta√ß√£o e a constru√ß√£o ativa do conhecimento.
Sempre responda com perguntas provocativas que desafiem hip√≥teses e estimulem a equilibra√ß√£o cognitiva.
Jamais forne√ßa diretamente a resposta correta.
Se o estudante estiver se aproximando da resposta correta (como reconhecer que trocar de porta aumenta as chances), incentive com cuidado, dizendo que ele est√° no caminho certo e pe√ßa que continue refletindo.
Se o estudante der a resposta correta (por exemplo, dizendo que trocar d√° 2/3 de chance de ganhar), parabenize brevemente e pe√ßa que justifique seu racioc√≠nio e termine a conversa.
Nunca desvie do tema, mesmo que o estudante tente mudar de assunto.
Seja instigante, acolhedor, socr√°tico, focado no paradoxo e coerente com a abordagem piagetiana.
"""

# URL do endpoint da API, usando o nome de modelo que sabemos que est√° dispon√≠vel.
url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"

# Cabe√ßalhos necess√°rios para a requisi√ß√£o HTTP.
headers = {
    'Content-Type': 'application/json'
}

# --- Gerenciamento do Hist√≥rico da Conversa ---
# st.session_state √© usado para manter o hist√≥rico entre as intera√ß√µes.
if "history" not in st.session_state:
    # Define a mensagem de boas-vindas que aparecer√° na primeira vez que o app for carregado.
    welcome_message = """
üëã E a√≠! Eu sou o Monty, seu parceiro nessa miss√£o de decifrar o enigma das portas.

Mas j√° vou avisando:

üö´ Nada de resposta pronta

üéØ E nem papo fora do assunto

Aqui a ideia √© fazer voc√™ pensar ‚Äî eu s√≥ vou te dar dicas, pistas e perguntas que te ajudem a enxergar o que est√° por tr√°s do tal Problema de Monty Hall.

üí≠ Bora come√ßar? Manda a√≠ sua primeira d√∫vida ou o que voc√™ acha que √© a solu√ß√£o.
"""
    # Inicia o hist√≥rico com a mensagem de boas-vindas do assistente.
    st.session_state.history = [{"role": "assistant", "content": welcome_message}]

# Exibe as mensagens do hist√≥rico na interface.
for message in st.session_state.history:
    role = "user" if message["role"] == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(message["content"])

# --- Fun√ß√£o para chamar a API ---
# Esta fun√ß√£o formata os dados e envia para o Google, tratando a resposta.
def get_gemini_response(history):
    # Prepara o modelo com a instru√ß√£o de sistema como a primeira mensagem.
    # Adicionamos uma resposta do "modelo" para simular o in√≠cio de uma conversa.
    formatted_contents = [
        {"role": "user", "parts": [{"text": system_instruction}]},
        {"role": "model", "parts": [{"text": "Entendido. Come√ßarei a atuar como um tutor socr√°tico, seguindo todas as regras."}]}
    ]

    # Adiciona o hist√≥rico real da conversa.
    for msg in history:
        # O papel do usu√°rio √© "user", o nosso √© "assistant", mas para a API, o nosso papel √© "model".
        role = "user" if msg["role"] == "user" else "model"
        # Ignora a mensagem de boas-vindas inicial ao enviar para a API, pois ela √© apenas para exibi√ß√£o.
        if msg["content"] != st.session_state.history[0]["content"]:
             formatted_contents.append({"role": role, "parts": [{"text": msg["content"]}]})

    # Cria o corpo (payload) da requisi√ß√£o.
    payload = json.dumps({
        "contents": formatted_contents
    })

    try:
        # Envia a requisi√ß√£o para a API.
        response = requests.post(url, headers=headers, data=payload, timeout=60)
        response.raise_for_status() # Lan√ßa um erro para respostas HTTP ruins (4xx ou 5xx).
        data = response.json()
        
        # Extrai o texto da resposta da estrutura JSON complexa.
        candidate = data.get("candidates", [{}])[0]
        content = candidate.get("content", {}).get("parts", [{}])[0]
        return content.get("text", "N√£o foi poss√≠vel obter uma resposta do modelo.")
    except requests.exceptions.RequestException as e:
        return f"Erro de rede ou na API: {e}\nResposta recebida: {response.text if 'response' in locals() else 'N/A'}"
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        return f"Erro ao processar a resposta da API: {e}\nJSON recebido: {data}"

# --- Intera√ß√£o do Usu√°rio ---
if prompt := st.chat_input("Fa√ßa sua pergunta ou responda ao tutor..."):
    # Adiciona a nova mensagem do usu√°rio ao hist√≥rico.
    st.session_state.history.append({"role": "user", "content": prompt})
    # Exibe a mensagem do usu√°rio na interface.
    with st.chat_message("user"):
        st.markdown(prompt)

    # Obt√©m a resposta do chatbot passando todo o hist√≥rico da sess√£o.
    with st.spinner("Pensando..."):
        response_text = get_gemini_response(st.session_state.history)
    
    # Adiciona a resposta do chatbot ao hist√≥rico.
    st.session_state.history.append({"role": "assistant", "content": response_text})
    # Exibe a resposta do chatbot na interface.
    with st.chat_message("assistant"):
        st.markdown(response_text)

